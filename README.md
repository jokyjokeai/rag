# RAG Local System - Intelligent Knowledge Base

[![GitHub](https://img.shields.io/badge/github-jokyjokeai%2Frag-blue?logo=github)](https://github.com/jokyjokeai/rag)
[![Python](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)
[![Status](https://img.shields.io/badge/status-95%25%20complete-success)](documentation/PROJECT_STATUS.md)

SystÃ¨me RAG (Retrieval-Augmented Generation) local et intelligent pour l'ingestion, le traitement et l'interrogation de contenus techniques multi-sources.

ğŸ‰ **Version 1.0 - Production Ready** | [Documentation complÃ¨te](documentation/) | [GitHub Repo](https://github.com/jokyjokeai/rag)

## ğŸ¯ CaractÃ©ristiques

- **100% Local & Open Source** (sauf Brave Search API - 2000 req/mois gratuit)
- **DÃ©tection automatique** : URLs vs prompts texte
- **Multi-sources** : YouTube (chaÃ®nes/vidÃ©os), GitHub (repos), Sites web (documentation)
- **Chunking intelligent** adaptÃ© au type de contenu
- **Enrichissement mÃ©tadonnÃ©es** via Ollama (LLM local)
- **Pas de doublons** : SystÃ¨me de hash pour Ã©viter les duplicatas
- **Queue systÃ¨me** : Traitement par batch avec priorisation
- **Refresh automatique** : Maintien Ã  jour hebdomadaire
- **Interface MCP** : Compatible Claude Code et chat custom

## âš™ï¸ Comportement du crawling

Le systÃ¨me dÃ©cide automatiquement de crawler ou scraper selon le type de site :

### ğŸ•·ï¸ Sites crawlÃ©s (dÃ©couverte automatique de toutes les pages)

**Sites de documentation :**
- Domaines : `docs.*`, `doc.*`, `wiki`, `confluence`
- Plateformes : `readthedocs`, `gitbook`, `notion.site`, `readme.io`
- Guides : URLs contenant `/tutorial`, `/guide`, `/learn`
- Blogs : URLs contenant `/blog`, `/article`, `/post`, `/news`

**Exemples :**
- âœ… `https://docs.asterisk.org` â†’ Crawl jusqu'Ã  1000 pages
- âœ… `https://fastapi.tiangolo.com/tutorial/` â†’ Crawl complet de la section
- âœ… `https://docs.python.org` â†’ DÃ©couverte de toute la documentation
- âœ… `https://example.com/blog` â†’ Crawl de tous les articles

**RÃ©sultat :** DÃ©couvre 50-1000 pages automatiquement, les ajoute Ã  la queue pour scraping ultÃ©rieur.

### ğŸ“„ Sites scrapÃ©s (page unique seulement)

**Tous les autres sites web** qui ne correspondent pas aux patterns ci-dessus.

**Exemples :**
- âš ï¸ `https://company.com/product` â†’ Scrape de cette page uniquement
- âš ï¸ `https://blog.example.com/article-123` â†’ Page unique
- âš ï¸ `https://github.com/user/repo` â†’ README + dossier docs

**RÃ©sultat :** Extrait le contenu de cette URL seulement, pas de dÃ©couverte de pages liÃ©es.

### ğŸ’¡ Astuce

Pour les sites qui ne sont pas dÃ©tectÃ©s comme documentation mais que vous souhaitez indexer entiÃ¨rement :
1. Ajoutez manuellement les URLs des pages importantes
2. Ou utilisez une recherche par prompt (ex: "tutoriels FastAPI") qui dÃ©couvrira automatiquement du contenu

## ğŸ› Corrections rÃ©centes (v1.0)

### Bugs critiques corrigÃ©s
- **Quota tracking** : Correction du format datetime (SQLite compatibility)
- **Seuil de recherche** : Ajustement du threshold de 0.3 Ã  1.5 pour plus de rÃ©sultats
- **RequÃªtes concurrentes** : Ajout d'un flag optionnel `ENABLE_COMPETITOR_QUERIES`

### AmÃ©liorations
- Meilleure documentation avec .env.example
- Structure de projet organisÃ©e (dossier documentation/)
- Suppression de tous les scripts de test temporaires
- Configuration Git complÃ¨te (.gitignore)

## ğŸ—ï¸ Architecture

```
Input (URLs ou Prompt)
    â†“
Orchestrator (dÃ©tection + Ollama + Brave Search)
    â†“
URL Discovery Layer (SQLite - pas de doublons)
    â†“
Queue Manager (priorisation + batch)
    â†“
Scrapers (YouTube, GitHub, Web)
    â†“
Processing (chunking + embeddings + enrichissement)
    â†“
Vector Database (ChromaDB)
    â†“
MCP Server (search_rag, add_source, get_status)
```

## ğŸš€ Installation

### PrÃ©requis

- Python 3.11+
- Ollama installÃ© et en cours d'exÃ©cution
- ClÃ©s API (optionnelles) : Brave Search, YouTube, GitHub

### Installation

```bash
# 1. Clone et navigation
cd rag-local-system

# 2. Environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# 3. DÃ©pendances
pip install -r requirements.txt

# 4. Playwright (pour sites web dynamiques)
playwright install

# 5. Configuration
cp .env.example .env
# Ã‰diter .env avec vos clÃ©s API

# 6. Ollama
ollama pull llama3.2
```

## âš™ï¸ Configuration (.env)

```bash
# APIs (optionnelles mais recommandÃ©es)
BRAVE_API_KEY=votre_cle_brave
YOUTUBE_API_KEY=votre_cle_youtube
GITHUB_TOKEN=votre_token_github

# Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.2

# Chemins
CHROMA_DB_PATH=./data/chroma_db
SQLITE_DB_PATH=./data/discovered_urls.db

# Processing
BATCH_SIZE=10
CONCURRENT_WORKERS=3
MAX_CHUNK_SIZE=512
```

## ğŸ“– Utilisation

### Mode URL direct

```python
from orchestrator import Orchestrator

orch = Orchestrator()

# Ajouter des URLs directement
result = orch.process_input("""
https://fastapi.tiangolo.com
https://github.com/tiangolo/fastapi
https://www.youtube.com/@ArjanCodes
""")

print(f"âœ… {result['urls_added']} URLs ajoutÃ©es")
```

### Mode Prompt (recherche web)

```python
from orchestrator import Orchestrator

orch = Orchestrator()

# Prompt textuel - Ollama analyse + Brave Search
result = orch.process_input("Je veux apprendre FastAPI avec PostgreSQL")

print(f"ğŸ” {result['urls_discovered']} URLs dÃ©couvertes")
print(f"âœ… {result['urls_added']} URLs ajoutÃ©es")
```

## ğŸ”§ Composants

### 1. Orchestrator
- DÃ©tecte type d'entrÃ©e (URL vs prompt)
- Analyse prompts avec Ollama
- Recherche web via Brave Search API
- Ajoute URLs Ã  la base de donnÃ©es

### 2. Database Layer
- **SQLite** : `discovered_urls` avec dÃ©tection doublons
- **ChromaDB** : Stockage vectoriel des chunks

### 3. Crawlers (Ã  venir)
- YouTube : DÃ©couverte vidÃ©os depuis chaÃ®nes
- GitHub : Listing fichiers repos
- Web : Crawl rÃ©cursif avec limite profondeur

### 4. Scrapers (Ã  venir)
- YouTube : Transcriptions via `youtube-transcript-api`
- GitHub : Code + docs via PyGithub
- Web : HTML â†’ Markdown

### 5. Processing Pipeline (Ã  venir)
- Chunking intelligent par type
- Embeddings locaux (`sentence-transformers`)
- Enrichissement mÃ©tadonnÃ©es (Ollama)

### 6. MCP Server (Ã  venir)
- `search_rag` : Recherche sÃ©mantique
- `add_source` : Ajout URLs
- `get_source_status` : Stats systÃ¨me

## ğŸ“Š Statut du projet

âœ… **Production Ready (95% complÃ©tÃ©)**

**Composants opÃ©rationnels :**
- âœ… Orchestrator (dÃ©tection URL/prompt, Brave Search, Ollama)
- âœ… Database layer (SQLite + ChromaDB)
- âœ… Scrapers (YouTube, GitHub, Web) avec crawlers
- âœ… Processing pipeline (chunking, embeddings, enrichissement)
- âœ… Queue manager (batch async processing)
- âœ… MCP server (Claude Desktop integration)
- âœ… CLI interface (10 commandes interactives)
- âœ… Auto-refresh scheduler
- âœ… Rate limiting & quota tracking

**DÃ©tails du projet :**
- 13,164 lignes de code Python
- 432 chunks dans ChromaDB
- 18 fichiers de documentation
- Architecture modulaire et extensible

## ğŸ“ Exemple complet

```python
# Initialisation
from orchestrator import Orchestrator

orch = Orchestrator()

# Cas 1: URLs directes
orch.process_input("https://fastapi.tiangolo.com")

# Cas 2: Prompt textuel
orch.process_input("Apprendre FastAPI async")

# Statistiques
stats = orch.get_stats()
print(stats)

# Fermeture
orch.close()
```

## ğŸ”’ Respect & Ã‰thique

- âœ… Respect `robots.txt`
- âœ… Rate limiting (1 req/sec par domaine)
- âœ… User-Agent identifiable
- âœ… Retry avec backoff exponentiel
- âœ… Respect des limites API

## ğŸ“š Stack technique

- **LLM** : Ollama (llama3.2)
- **Vector DB** : ChromaDB
- **Database** : SQLite
- **Embeddings** : sentence-transformers
- **Web scraping** : Playwright, BeautifulSoup
- **YouTube** : youtube-transcript-api
- **GitHub** : PyGithub
- **Processing** : LangChain, tiktoken

## ğŸ¤ Contribution

Contributions bienvenues ! Le projet suit une architecture modulaire et extensible.

### Comment contribuer

1. Fork le projet
2. CrÃ©ez une branche pour votre feature (`git checkout -b feature/AmazingFeature`)
3. Committez vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrez une Pull Request

### Guidelines

- Code Python conforme Ã  PEP 8
- Documentation en franÃ§ais ou anglais
- Tests pour les nouvelles fonctionnalitÃ©s
- Logs structurÃ©s avec loguru

## ğŸ“„ Licence

MIT License - voir fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ™ Remerciements

- [Ollama](https://ollama.ai/) pour l'infÃ©rence LLM locale
- [ChromaDB](https://www.trychroma.com/) pour la base vectorielle
- [Brave Search](https://brave.com/search/api/) pour l'API de recherche web
- CommunautÃ© open source pour les bibliothÃ¨ques utilisÃ©es

---

**Projet dÃ©veloppÃ© avec Claude Code** | [Documentation](documentation/) | [Issues](https://github.com/jokyjokeai/rag/issues)
