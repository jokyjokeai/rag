# Comportement du crawling - Guide dÃ©taillÃ©

## Vue d'ensemble

Le systÃ¨me RAG dÃ©cide automatiquement **quand crawler** (dÃ©couvrir toutes les pages d'un site) vs **quand scraper** (extraire le contenu d'une seule page).

---

## DÃ©cision automatique : Crawl vs Scrape

### Flux de dÃ©cision

```
URL ajoutÃ©e Ã  la queue
    â†“
Est-ce un site web ?
    â”œâ”€ NON (YouTube/GitHub) â†’ Scrape/Traitement spÃ©cifique
    â””â”€ OUI â†’ Est-ce dÃ©couvert d'un crawl prÃ©cÃ©dent ?
        â”œâ”€ OUI â†’ Scrape direct (Ã©vite crawl rÃ©cursif)
        â””â”€ NON â†’ Correspond aux patterns de documentation ?
            â”œâ”€ OUI â†’ CRAWL (dÃ©couvre 1-1000 pages)
            â””â”€ NON â†’ SCRAPE (page unique)
```

---

## Patterns de dÃ©tection du crawling

### ğŸ•·ï¸ Sites crawlÃ©s automatiquement

#### 1. Domaines de documentation
- `docs.example.com` - Sous-domaine "docs"
- `doc.example.com` - Sous-domaine "doc"
- `documentation.example.com` - Sous-domaine "documentation"

#### 2. Plateformes de documentation
- `*.readthedocs.io` - ReadTheDocs
- `*.gitbook.io` - GitBook
- `*.notion.site` - Notion sites publics
- `*.readme.io` - ReadMe.io

#### 3. Wikis et Confluence
- Domaine contient `wiki`
- Domaine contient `confluence`

#### 4. Guides et tutoriels (dans le path)
- `/tutorial/` - Tutoriels
- `/guide/` - Guides
- `/learn/` - Sections d'apprentissage

#### 5. Blogs (dans le path)
- `/blog/` - Blogs
- `/article/` - Articles
- `/post/` - Posts
- `/news/` - News

### Exemples concrets

| URL | CrawlÃ© ? | Raison |
|-----|----------|--------|
| `https://docs.python.org` | âœ… Oui | Domaine contient "docs" |
| `https://fastapi.tiangolo.com/tutorial/` | âœ… Oui | Path contient "tutorial" |
| `https://wiki.archlinux.org` | âœ… Oui | Domaine contient "wiki" |
| `https://example.com/blog/article-1` | âœ… Oui | Path contient "blog" |
| `https://company.com/product` | âŒ Non | Aucun pattern |
| `https://signalwire.com/freeswitch` | âŒ Non | Aucun pattern |
| `https://blog.example.com/2024/article` | âŒ Non | "blog" dans domaine, pas dans path |

**Note importante** : Le pattern doit Ãªtre dans le **path** pour `/blog`, `/tutorial`, etc., pas seulement dans le domaine.

---

## Processus de crawling

### Phase 1 : DÃ©couverte des pages

1. **DÃ©marrage** : Le crawler visite l'URL de dÃ©part
2. **Extraction des liens** : Parse tous les liens `<a href="...">`
3. **Filtrage** :
   - MÃªme domaine uniquement (par dÃ©faut)
   - Exclut les fichiers (images, PDF, ZIP, etc.)
   - Exclut les patterns non-pertinents (`/login`, `/search`, `/admin`)
4. **Queue** : Ajoute les nouveaux liens Ã  visiter
5. **RÃ©cursion** : RÃ©pÃ¨te jusqu'Ã  atteindre `max_pages` (default: 1000)

### Phase 2 : Ajout Ã  la queue

- Toutes les pages dÃ©couvertes sont ajoutÃ©es Ã  la base de donnÃ©es
- **Status** : `pending` (en attente de scraping)
- **Source** : `discovered_from='website_crawl:URL_ORIGINE'`
- **Priority** : 50 (prioritÃ© moyenne)
- **DÃ©doublonnage** : URLs dÃ©jÃ  en base sont skip

### Phase 3 : Scraping ultÃ©rieur

- Les pages dÃ©couvertes seront traitÃ©es dans le prochain batch
- Chaque page est scrapÃ©e individuellement (pas de re-crawl)
- Le contenu est chunkÃ©, embedÃ© et stockÃ© dans ChromaDB

---

## Limites et paramÃ¨tres

### ParamÃ¨tres configurables

| ParamÃ¨tre | Valeur par dÃ©faut | Description |
|-----------|-------------------|-------------|
| `max_pages` | 1000 | Nombre maximum de pages Ã  dÃ©couvrir |
| `same_domain_only` | `True` | Rester sur le mÃªme domaine |
| `timeout` | 10000 ms | Timeout de chargement par page |
| `delay` | 0.5s | DÃ©lai entre chaque page |

### Fichiers exclus automatiquement

**Extensions ignorÃ©es** :
- Images : `.jpg`, `.jpeg`, `.png`, `.gif`, `.svg`, `.webp`
- VidÃ©os : `.mp4`, `.avi`, `.mov`
- Documents : `.pdf`, `.zip`, `.tar`, `.gz`, `.rar`
- ExÃ©cutables : `.exe`, `.dmg`, `.iso`

**Paths ignorÃ©s** :
- `/search`, `/login`, `/signup`
- `/cart`, `/checkout`, `/account`
- `/admin`, `/api/`

---

## Cas d'usage

### Cas 1 : Documentation officielle

**Input** :
```
https://docs.fastapi.tiangolo.com
```

**RÃ©sultat** :
- âœ… DÃ©tectÃ© comme doc (domaine "docs")
- ğŸ•·ï¸ Crawl de toute la documentation
- ğŸ“„ DÃ©couvre ~200-500 pages
- â±ï¸ DurÃ©e : 3-8 minutes
- ğŸ’¾ Toutes les pages ajoutÃ©es Ã  la queue

### Cas 2 : Article de blog unique

**Input** :
```
https://company.com/blog/my-article
```

**RÃ©sultat** :
- âœ… DÃ©tectÃ© comme blog (path "/blog")
- ğŸ•·ï¸ Crawl du blog entier
- ğŸ“„ DÃ©couvre tous les articles
- âš ï¸ **Attention** : Va crawler TOUS les articles, pas juste celui-ci

### Cas 3 : Site web gÃ©nÃ©rique

**Input** :
```
https://signalwire.com/freeswitch
```

**RÃ©sultat** :
- âŒ Pas de pattern dÃ©tectÃ©
- ğŸ“„ Scrape de cette page uniquement
- â±ï¸ DurÃ©e : 1-3 secondes
- ğŸ’¾ 1 chunk ajoutÃ© Ã  ChromaDB

### Cas 4 : GitHub Repository

**Input** :
```
https://github.com/user/repo
```

**RÃ©sultat** :
- ğŸ”§ Traitement spÃ©cial GitHub
- ğŸ“„ Clone sparse (README + docs/)
- ğŸ“š Pas de crawling web
- ğŸ’¾ Fichiers markdown + code pertinent

---

## StratÃ©gies pour les sites non-crawlÃ©s

Si un site important n'est **pas** dÃ©tectÃ© comme documentation :

### Option 1 : Ajouter les URLs manuellement

```python
# Ajouter les pages une par une
urls = [
    "https://site.com/page1",
    "https://site.com/page2",
    "https://site.com/page3"
]
for url in urls:
    rag.add_sources(url)
```

### Option 2 : Utiliser la recherche par prompt

```python
# Le systÃ¨me dÃ©couvrira automatiquement des URLs pertinentes
rag.add_sources("tutoriels complets sur FreeSWITCH")
```

Le prompt dÃ©clenchera :
1. Analyse Ollama â†’ gÃ©nÃ©ration de requÃªtes de recherche
2. Brave Search â†’ dÃ©couverte d'URLs
3. Filtrage â†’ sÃ©lection des URLs pertinentes
4. Crawl automatique des docs trouvÃ©es

### Option 3 : Modifier les patterns (dÃ©veloppeurs)

**Fichier** : `scrapers/web_crawler.py` (ligne 184-230)

Ajouter votre domaine/pattern :
```python
# Documentation sites - always crawl
doc_patterns = [
    'docs.', 'doc.', 'documentation',
    'wiki', 'confluence',
    'readthedocs', 'gitbook',
    'YOUR_SITE_PATTERN'  # Ajoutez ici
]
```

---

## FAQ

### Q : Pourquoi ne pas crawler tous les sites ?

**R** : Raisons techniques et pratiques :
1. **Performance** : Un site peut avoir des milliers de pages
2. **Pertinence** : Beaucoup de pages ne sont pas du contenu (login, contact, etc.)
3. **Ressources** : Limite le temps de traitement et l'espace disque
4. **QualitÃ©** : Les docs structurÃ©es ont un meilleur signal/bruit

### Q : Comment savoir si un site sera crawlÃ© ?

**R** : VÃ©rifiez si l'URL contient :
- `docs.` ou `doc.` dans le domaine
- `wiki`, `confluence` dans le domaine
- `/tutorial`, `/guide`, `/blog` dans le path
- Ou une plateforme reconnue (readthedocs, gitbook, etc.)

### Q : Puis-je forcer le crawling d'un site ?

**R** : Actuellement, non. Le comportement est automatique. Solutions de contournement :
1. Ajouter manuellement les URLs importantes
2. Utiliser une recherche par prompt
3. Modifier les patterns dans le code (pour dÃ©veloppeurs)

### Q : Combien de temps prend un crawl ?

**R** : DÃ©pend du nombre de pages :
- 50 pages : ~30 secondes - 1 minute
- 200 pages : ~2-4 minutes
- 1000 pages : ~8-15 minutes

**Formule approximative** : `temps = nombre_pages Ã— 0.5s`

### Q : Le crawl peut-il dÃ©passer 1000 pages ?

**R** : Non, limite fixÃ©e Ã  1000 pages par crawl pour Ã©viter :
- Temps de traitement excessif
- Surcharge rÃ©seau
- ProblÃ¨mes de mÃ©moire

Si un site a plus de 1000 pages, seules les 1000 premiÃ¨res dÃ©couvertes seront indexÃ©es.

---

## Logs et feedback

### Logs de progression

Pendant un crawl, vous verrez :

```
ğŸ•·ï¸  Crawling: https://docs.example.com (max: 1000 pages)

ğŸ“„ [1/1000] https://docs.example.com/index.html...
   â†’ 45 links found | Queue: 35 | Visited: 1

ğŸ“„ [10/1000] https://docs.example.com/getting-started...
   â†’ 23 links found | Queue: 143 | Visited: 10

ğŸ”„ Progress: 20/1000 pages | Queue: 62 | Elapsed: 15s | ETA: ~8min

ğŸ“„ [50/1000] https://docs.example.com/advanced/...
   â†’ 18 links found | Queue: 287 | Visited: 50

âœ… Crawling complete: discovered 243 pages in 3m 24s

ğŸ’¾ Adding 243 discovered pages to database...
   [50/243] Added 44 new, skipped 6 duplicates
   [100/243] Added 88 new, skipped 12 duplicates

âœ… Website crawled successfully!
```

### Message pour les sites non-crawlÃ©s

```
â„¹ï¸  Single page scrape (not detected as documentation site)
   ğŸ’¡ Crawling triggers for: docs.*, wiki, tutorial, blog, readthedocs, etc.
Scraping web page: https://example.com/page
```

---

## Fichiers de code impliquÃ©s

| Fichier | RÃ´le |
|---------|------|
| `scrapers/web_crawler.py` | Logique de crawling et patterns de dÃ©tection |
| `queue_processor/integrated_processor.py` | DÃ©cision crawl vs scrape |
| `database/models.py` | Stockage des URLs dÃ©couvertes |
| `config/settings.py` | Configuration (futurs paramÃ¨tres) |

---

## AmÃ©liorations futures envisagÃ©es

- [ ] Configuration : `WEBSITE_CRAWL_MODE = "documentation_only" | "all" | "none"`
- [ ] Choix utilisateur : "Voulez-vous crawler ce site ?" (mode interactif)
- [ ] Liste blanche/noire de domaines
- [ ] DÃ©tection intelligente via sitemap.xml
- [ ] Analyse de robots.txt pour les limites
- [ ] Crawl par profondeur (depth-first vs breadth-first)
- [ ] Crawler les sites dynamiques (SPA/React)

---

**DerniÃ¨re mise Ã  jour** : Version 1.0 (2025-11-16)
